name: ML Pipeline CI/CD

on:
  pull_request:
    branches: [ main ]

permissions:
  contents: write
  pull-requests: write
  issues: write
  
jobs:
  test-and-validate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run data validation tests
      run: |
        pytest tests/test_data_validation.py -v --tb=short
    
    - name: Run model evaluation tests
      run: |
        pytest tests/test_model_evaluation.py -v --tb=short
    
    - name: Run hyperparameter tuning with MLFlow
      run: |
        python src/hyperparameter_tuning.py
        
    - name: Train baseline model (for comparison)
      run: |
        python src/train.py
    
    - name: Evaluate model
      run: |
        python src/evaluate.py
        
    - name: Install Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        
    - name: Install CML 
      run: npm install -g @dvcorg/cml
    
    - name: Generate ML Report with MLFlow Results
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Create ML report
        echo "# IRIS Model Performance Report" > report.md
        echo "## Hyperparameter Tuning Results" >> report.md
        
        # Extract best parameters if available
        python << 'EOF' >> report.md
        import yaml
        import os
        import json
        
        # Check for best parameters from hyperparameter tuning
        if os.path.exists('models/best_params.yaml'):
            with open('models/best_params.yaml', 'r') as f:
                best_params = yaml.safe_load(f)
            
            print(f"")
            print(f"### Best Hyperparameters Found")
            print(f"| Parameter | Value |")
            print(f"|-----------|-------|")
            for param, value in best_params.items():
                print(f"| {param} | {value} |")
            print(f"")
        else:
            print("No hyperparameter tuning results found.")
        EOF
        
        echo "## Model Metrics" >> report.md
        
        # Extract metrics from JSON file
        python << 'EOF' >> report.md
        import json
        import os
        
        if os.path.exists('models/metrics.json'):
            with open('models/metrics.json', 'r') as f:
                data = json.load(f)
            
            metrics = data['metrics']
            print(f"")
            print(f"| Metric | Value |")
            print(f"|--------|-------|")
            print(f"| Accuracy | {metrics['accuracy']:.4f} |")
            print(f"| Precision | {metrics['precision']:.4f} |")
            print(f"| Recall | {metrics['recall']:.4f} |")
            print(f"| F1-Score | {metrics['f1_score']:.4f} |")
            print(f"")
            
            # Classification report
            print(f"## Detailed Classification Report")
            print(f"```")
            class_report = data['classification_report']
            for class_name, class_metrics in class_report.items():
                if isinstance(class_metrics, dict) and 'precision' in class_metrics:
                    print(f"{class_name}: precision={class_metrics['precision']:.3f}, recall={class_metrics['recall']:.3f}, f1-score={class_metrics['f1-score']:.3f}")
            print(f"```")
        else:
            print("Metrics file not found!")
        EOF
        
        # Add MLFlow information
        echo "## MLFlow Tracking" >> report.md
        echo "- **Tracking URI**: sqlite:///mlflow.db" >> report.md
        echo "- **Experiment**: iris_hyperparameter_tuning" >> report.md
        echo "- **Hyperparameter Search Space**: RandomForestClassifier parameters" >> report.md
        
        # Add data validation summary
        echo "## Data Validation Status" >> report.md
        echo "✅ Data validation tests passed" >> report.md
        echo "✅ Model evaluation tests passed" >> report.md
        echo "✅ Hyperparameter tuning completed" >> report.md
        
        # Add model info
        echo "## Model Configuration" >> report.md
        echo "- Algorithm: Random Forest Classifier (with hyperparameter tuning)" >> report.md
        echo "- Features: 4 (sepal length, sepal width, petal length, petal width)" >> report.md
        echo "- Classes: 3 (setosa, versicolor, virginica)" >> report.md
        echo "- Training samples: 120" >> report.md
        echo "- Test samples: 30" >> report.md
        
        # Publish report as comment
        cml comment create report.md
    
    - name: Archive MLFlow artifacts
      uses: actions/upload-artifact@v3
      with:
        name: mlflow-artifacts
        path: |
          mlflow.db
          mlruns/
          models/best_params.yaml
        retention-days: 30

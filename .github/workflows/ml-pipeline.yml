name: ML Pipeline CI/CD

on:
  push:
    branches: [ dev, main ]
  pull_request:
    branches: [ main ]

jobs:
  test-and-validate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run data validation tests
      run: |
        pytest tests/test_data_validation.py -v --tb=short
    
    - name: Run model evaluation tests
      run: |
        pytest tests/test_model_evaluation.py -v --tb=short
    
    - name: Train model
      run: |
        python src/train.py
    
    - name: Evaluate model
      run: |
        python src/evaluate.py
    
    - name: Setup CML
      uses: iterative/setup-cml@v1
    
    - name: Generate ML Report
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Create ML report
        echo "# IRIS Model Performance Report" > report.md
        echo "## Model Metrics" >> report.md
        
        # Extract metrics from JSON file
        python << 'EOF' >> report.md
        import json
        import os
        
        if os.path.exists('models/metrics.json'):
            with open('models/metrics.json', 'r') as f:
                data = json.load(f)
            
            metrics = data['metrics']
            print(f"")
            print(f"| Metric | Value |")
            print(f"|--------|-------|")
            print(f"| Accuracy | {metrics['accuracy']:.4f} |")
            print(f"| Precision | {metrics['precision']:.4f} |")
            print(f"| Recall | {metrics['recall']:.4f} |")
            print(f"| F1-Score | {metrics['f1_score']:.4f} |")
            print(f"")
            
            # Classification report
            print(f"## Detailed Classification Report")
            print(f"```")
            class_report = data['classification_report']
            for class_name, class_metrics in class_report.items():
                if isinstance(class_metrics, dict) and 'precision' in class_metrics:
                    print(f"{class_name}: precision={class_metrics['precision']:.3f}, recall={class_metrics['recall']:.3f}, f1-score={class_metrics['f1-score']:.3f}")
            print(f"```")
        else:
            print("Metrics file not found!")
        EOF
        
        # Add confusion matrix if it exists
        if [ -f "models/plots/confusion_matrix.png" ]; then
          echo "## Confusion Matrix" >> report.md
          echo "![Confusion Matrix](models/plots/confusion_matrix.png)" >> report.md
          cml-publish models/plots/confusion_matrix.png --md >> report.md
        fi
        
        # Add data validation summary
        echo "## Data Validation Status" >> report.md
        echo "✅ Data validation tests passed" >> report.md
        echo "✅ Model evaluation tests passed" >> report.md
        
        # Add model info
        echo "## Model Configuration" >> report.md
        echo "- Algorithm: Random Forest Classifier" >> report.md
        echo "- Features: 4 (sepal length, sepal width, petal length, petal width)" >> report.md
        echo "- Classes: 3 (setosa, versicolor, virginica)" >> report.md
        echo "- Training samples: 120" >> report.md
        echo "- Test samples: 30" >> report.md
        
        # Publish report as comment
        cml-send-comment report.md
